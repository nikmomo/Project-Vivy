{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Use: *RNN w/ LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import ast\n",
    "\n",
    "file_path = 'normalized_output.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "data['ph_seq_encoded'] = data['ph_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['ph_dur'] = data['ph_dur'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['f0_seq'] = data['f0_seq'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else x)  # Handle NaN for f0_seq\n",
    "data['note_seq_encoded'] = data['note_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Normalize the float arrays\n",
    "scaler_ph_dur = MinMaxScaler(feature_range=(0, 1))\n",
    "# Normalize ph_dur assuming they are already in the correct format\n",
    "data['ph_dur'] = [scaler_ph_dur.fit_transform(np.array(seq).reshape(-1, 1)).flatten() for seq in data['ph_dur']]\n",
    "\n",
    "# Only normalize f0_seq if it's not NaN\n",
    "scaler_f0_seq = MinMaxScaler(feature_range=(0, 1))\n",
    "data['f0_seq'] = [scaler_f0_seq.fit_transform(np.array(seq).reshape(-1, 1)).flatten() if seq is not np.nan else np.nan for seq in data['f0_seq']]\n",
    "\n",
    "# Find the maximum sequence length across all sequence columns\n",
    "max_sequence_length = max(\n",
    "    max(data['ph_seq_encoded'].apply(len)),\n",
    "    max(data['ph_dur'].apply(len)),\n",
    "    max([len(seq) for seq in data['f0_seq'] if seq is not np.nan]),  # Only consider non-NaN sequences\n",
    "    max(data['note_seq_encoded'].apply(len))\n",
    ")\n",
    "\n",
    "# Pad the sequences\n",
    "data['ph_seq_encoded'] = pad_sequences(data['ph_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "data['ph_dur'] = pad_sequences(data['ph_dur'], maxlen=max_sequence_length, padding='post', dtype='float').tolist()\n",
    "data['f0_seq'] = [pad_sequences([seq], maxlen=max_sequence_length, padding='post', dtype='float').flatten() if seq is not np.nan else np.full(max_sequence_length, np.nan) for seq in data['f0_seq']]\n",
    "data['note_seq_encoded'] = pad_sequences(data['note_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "\n",
    "# Flatten all sequences into a single list\n",
    "all_ph_seq = [item for sublist in data['ph_seq_encoded'] for item in sublist]\n",
    "\n",
    "# Convert to numpy array and reshape to be 2D\n",
    "all_ph_seq_array = np.array(all_ph_seq).reshape(-1, 1)\n",
    "\n",
    "# Initialize and fit the OneHotEncoder on all sequences at once\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(all_ph_seq_array)\n",
    "\n",
    "# Now transform each sequence individually and store the transformed arrays\n",
    "data['ph_seq_encoded'] = [encoder.transform(np.array(seq).reshape(-1, 1)) for seq in data['ph_seq_encoded']]\n",
    "\n",
    "# Determine the number of categories for notes\n",
    "num_note_categories = max(data['note_seq_encoded'].apply(max)) + 1  # Assuming the sequences are zero-indexed\n",
    "\n",
    "# One-hot encode the note_seq_encoded\n",
    "data['note_seq_encoded'] = [to_categorical(seq, num_classes=num_note_categories) for seq in data['note_seq_encoded']]\n",
    "\n",
    "padded_note_seqs = np.array([pad_sequences([seq], maxlen=max_sequence_length, padding='post', value=0)[0] for seq in data['note_seq_encoded']])\n",
    "data['note_seq_encoded'] = list(padded_note_seqs)\n",
    "\n",
    "# After padding and encoding, convert to a numpy array\n",
    "X = np.array(data['ph_seq_encoded'].tolist())\n",
    "y_ph_dur = np.array(data['ph_dur'].tolist())\n",
    "y_note_seq_encoded = np.array(data['note_seq_encoded'].tolist())\n",
    "\n",
    "# Handle missing f0_seq data\n",
    "mask_value = -1  # Define a mask value that does not appear in the data\n",
    "data['f0_seq'] = np.array([np.full(max_sequence_length, mask_value) if np.isnan(seq).all() else seq for seq in data['f0_seq']])\n",
    "y_f0_seq = np.array(data['f0_seq'].tolist())\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_ph_dur_train, y_ph_dur_test, y_f0_seq_train, y_f0_seq_test, y_note_seq_encoded_train, y_note_seq_encoded_test = train_test_split(\n",
    "    X, y_ph_dur, y_f0_seq, y_note_seq_encoded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "14/41 [=========>....................] - ETA: 13:56 - loss: 3.6158 - ph_dur_output_loss: 0.0040 - f0_seq_output_loss: 0.3383 - note_seq_output_loss: 3.2734 - ph_dur_output_accuracy: 0.9953 - f0_seq_output_accuracy: 0.0011 - note_seq_output_accuracy: 0.8528"
     ]
    }
   ],
   "source": [
    "# Define the model with input layer and output layers\n",
    "input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "\n",
    "# Output layers for each prediction\n",
    "ph_dur_output = Dense(1, activation='linear', name='ph_dur_output')(lstm_layer)\n",
    "f0_seq_output = Dense(1, activation='linear', name='f0_seq_output')(lstm_layer)\n",
    "note_seq_output = Dense(y_note_seq_encoded_train.shape[2], activation='softmax', name='note_seq_output')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[ph_dur_output, f0_seq_output, note_seq_output])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss={'ph_dur_output': 'mse', 'f0_seq_output': 'mse', 'note_seq_output': 'categorical_crossentropy'}, metrics=['accuracy'])\n",
    "\n",
    "# Make sure your target arrays are in the correct shape before training\n",
    "y_note_seq_encoded_train = np.array([np.array(lst) if isinstance(lst, list) else lst for lst in y_note_seq_encoded_train])\n",
    "# Add an extra dimension to ph_dur and f0_seq if they are not already in the shape (num_samples, 1)\n",
    "# y_ph_dur_train = np.expand_dims(y_ph_dur_train, axis=-1)\n",
    "# y_f0_seq_train = np.expand_dims(y_f0_seq_train, axis=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, [y_ph_dur_train, y_f0_seq_train, y_note_seq_encoded_train], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Save the model and the scalers/encoders\n",
    "model.save('model.h5')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the scalers and encoders\n",
    "joblib.dump(scaler_ph_dur, 'scaler_ph_dur.pkl')\n",
    "joblib.dump(scaler_f0_seq, 'scaler_f0_seq.pkl')\n",
    "joblib.dump(encoder, 'ph_seq_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation\n",
    "Put user input and get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the encoding dictionary from the ph_token_to_int.json file\n",
    "with open('ph_token_to_int.json', 'r') as file:\n",
    "    ph_token_to_int = json.load(file)\n",
    "\n",
    "# Your input sequence\n",
    "input_sequence = \"n ei f a g e n a j i f u y u a p u AP n ei f a g e n a l e y u d ao en AP\"\n",
    "\n",
    "# Split the input sequence into individual phonemes\n",
    "input_phonemes = input_sequence.split()\n",
    "\n",
    "# Convert the phonemes to their corresponding integers using the encoding dictionary\n",
    "encoded_sequence = [ph_token_to_int[phoneme] for phoneme in input_phonemes]\n",
    "\n",
    "# Convert the sequence to a numpy array and pad it to the right length\n",
    "new_ph_seq_encoded = np.array([encoded_sequence])  # wrapping in a list to create a batch dimension\n",
    "new_ph_seq_encoded_padded = pad_sequences(new_ph_seq_encoded, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "input = np.expand_dims(new_ph_seq_encoded_padded, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 502ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_ph_dur, predicted_f0_seq, predicted_note_seq_encoded = model.predict(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[nan]]] \n",
      " [[[nan]]] \n",
      " [[[nan nan nan ... nan nan nan]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Niko\\Documents\\GitHub\\Song-Generation-Model-with-Vocal-Project-Vivy\\model_training.ipynb 单元格 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niko/Documents/GitHub/Song-Generation-Model-with-Vocal-Project-Vivy/model_training.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_ph_dur, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, predicted_f0_seq, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, predicted_note_seq_encoded)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Niko/Documents/GitHub/Song-Generation-Model-with-Vocal-Project-Vivy/model_training.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predicted_f0_seq \u001b[39m=\u001b[39m scaler_f0_seq\u001b[39m.\u001b[39;49minverse_transform(predicted_f0_seq)\n",
      "File \u001b[1;32mc:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:544\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    531\u001b[0m \n\u001b[0;32m    532\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 544\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    545\u001b[0m     X, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy, dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    548\u001b[0m X \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n\u001b[0;32m    549\u001b[0m X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n",
      "File \u001b[1;32mc:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\sklearn\\utils\\validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n\u001b[0;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m--> 951\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m     )\n\u001b[0;32m    956\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    957\u001b[0m     _assert_all_finite(\n\u001b[0;32m    958\u001b[0m         array,\n\u001b[0;32m    959\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    960\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    961\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    962\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "print(predicted_ph_dur, '\\n', predicted_f0_seq, '\\n', predicted_note_seq_encoded)\n",
    "\n",
    "predicted_f0_seq = scaler_f0_seq.inverse_transform(predicted_f0_seq)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectVivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
