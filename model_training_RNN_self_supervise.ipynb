{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Use: *RNN w/ LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "file_path = 'normalized_output.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4022, 4450, 65)\n",
      "y_ph_dur shape: (4022, 4450, 1)\n",
      "y_note_seq_encoded shape: (4022, 4450, 56)\n",
      "y_f0_seq shape: (4022, 4450, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "data['ph_seq_encoded'] = data['ph_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['ph_dur'] = data['ph_dur'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['f0_seq'] = data['f0_seq'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else x)  # Handle NaN for f0_seq\n",
    "data['note_seq_encoded'] = data['note_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Normalize the float arrays\n",
    "scaler_ph_dur = MinMaxScaler(feature_range=(0, 1))\n",
    "# Normalize ph_dur assuming they are already in the correct format\n",
    "data['ph_dur'] = [scaler_ph_dur.fit_transform(np.array(seq).reshape(-1, 1)).flatten() for seq in data['ph_dur']]\n",
    "\n",
    "# Only normalize f0_seq if it's not NaN\n",
    "scaler_f0_seq = MinMaxScaler(feature_range=(0, 1))\n",
    "data['f0_seq'] = [scaler_f0_seq.fit_transform(np.array(seq).reshape(-1, 1)).flatten() if seq is not np.nan else np.nan for seq in data['f0_seq']]\n",
    "\n",
    "# Find the maximum sequence length across all sequence columns\n",
    "max_sequence_length = max(\n",
    "    max(data['ph_seq_encoded'].apply(len)),\n",
    "    max(data['ph_dur'].apply(len)),\n",
    "    max([len(seq) for seq in data['f0_seq'] if seq is not np.nan]),  # Only consider non-NaN sequences\n",
    "    max(data['note_seq_encoded'].apply(len))\n",
    ")\n",
    "\n",
    "# Pad the sequences\n",
    "data['ph_seq_encoded'] = pad_sequences(data['ph_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "data['ph_dur'] = pad_sequences(data['ph_dur'], maxlen=max_sequence_length, padding='post', dtype='float').tolist()\n",
    "data['f0_seq'] = [pad_sequences([seq], maxlen=max_sequence_length, padding='post', dtype='float').flatten() if seq is not np.nan else np.full(max_sequence_length, np.nan) for seq in data['f0_seq']]\n",
    "data['note_seq_encoded'] = pad_sequences(data['note_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "\n",
    "# Flatten all sequences into a single list\n",
    "all_ph_seq = [item for sublist in data['ph_seq_encoded'] for item in sublist]\n",
    "\n",
    "# Convert to numpy array and reshape to be 2D\n",
    "all_ph_seq_array = np.array(all_ph_seq).reshape(-1, 1)\n",
    "del all_ph_seq\n",
    "\n",
    "# Initialize and fit the OneHotEncoder on all sequences at once\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(all_ph_seq_array)\n",
    "\n",
    "# Now transform each sequence individually and store the transformed arrays\n",
    "data['ph_seq_encoded'] = [encoder.transform(np.array(seq).reshape(-1, 1)) for seq in data['ph_seq_encoded']]\n",
    "\n",
    "# Determine the number of categories for notes\n",
    "num_note_categories = max(data['note_seq_encoded'].apply(max)) + 1  # Assuming the sequences are zero-indexed\n",
    "\n",
    "# One-hot encode the note_seq_encoded\n",
    "data['note_seq_encoded'] = [to_categorical(seq, num_classes=num_note_categories) for seq in data['note_seq_encoded']]\n",
    "\n",
    "padded_note_seqs = np.array([pad_sequences([seq], maxlen=max_sequence_length, padding='post', value=0)[0] for seq in data['note_seq_encoded']])\n",
    "data['note_seq_encoded'] = list(padded_note_seqs)\n",
    "\n",
    "# After padding and encoding, convert to a numpy array\n",
    "X = np.array(data['ph_seq_encoded'].tolist())\n",
    "y_ph_dur = np.array(data['ph_dur'].tolist())\n",
    "y_note_seq_encoded = np.array(data['note_seq_encoded'].tolist())\n",
    "\n",
    "# Ensure that all arrays have three dimensions\n",
    "y_ph_dur = y_ph_dur[..., np.newaxis]  # shape (num_sequences, sequence_length, 1)\n",
    "\n",
    "# Handle missing f0_seq data\n",
    "mask_value = -1  # Define a mask value that does not appear in the data\n",
    "y_f0_seq = np.array([np.full(max_sequence_length, mask_value) if np.isnan(seq).all() else seq for seq in data['f0_seq']])\n",
    "\n",
    "# If y_f0_seq is 2D, also convert it to 3D\n",
    "if y_f0_seq.ndim == 2:\n",
    "    y_f0_seq = y_f0_seq[..., np.newaxis]  # shape (num_sequences, sequence_length, 1)\n",
    "\n",
    "# Check the shapes of the arrays to make sure they are all 3D and can be concatenated\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y_ph_dur shape: {y_ph_dur.shape}\")\n",
    "print(f\"y_note_seq_encoded shape: {y_note_seq_encoded.shape}\")\n",
    "print(f\"y_f0_seq shape: {y_f0_seq.shape}\")\n",
    "\n",
    "# Concatenate your inputs along the last axis to create a single input array for each sequence\n",
    "X_combined = np.concatenate([X, y_ph_dur, y_note_seq_encoded, y_f0_seq], axis=-1)\n",
    "\n",
    "\n",
    "# Shift the combined input to create the input and target pairs\n",
    "X_input = X_combined[:, :-1, :]  # All but the last time step\n",
    "X_target = X_combined[:, 1:, :]  # All but the first time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some memory here\n",
    "del all_ph_seq_array\n",
    "del padded_note_seqs\n",
    "del all_ph_seq\n",
    "del data\n",
    "del X\n",
    "del y_f0_seq\n",
    "del y_ph_dur\n",
    "del y_note_seq_encoded\n",
    "del X_combined\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 2176s 35s/step - loss: -10.0123 - accuracy: 0.9338\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 2354s 37s/step - loss: -29.7561 - accuracy: 0.9952\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 2451s 39s/step - loss: -45.8949 - accuracy: 0.9952\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 2625s 42s/step - loss: -61.7230 - accuracy: 0.9952\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 2509s 40s/step - loss: -77.2914 - accuracy: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "input_layer = Input(shape=(X_input.shape[1], X_input.shape[2]))\n",
    "lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "output_layer = Dense(X_input.shape[2], activation='softmax')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the input and target sequences\n",
    "model.fit(X_input, X_target, epochs=5, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "model.save('model_self_supervised.h5')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# # Save the scalers and encoders\n",
    "# joblib.dump(scaler_ph_dur, 'scaler_ph_dur.pkl')\n",
    "# joblib.dump(scaler_f0_seq, 'scaler_f0_seq.pkl')\n",
    "# joblib.dump(encoder, 'ph_seq_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation\n",
    "Put user input and get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the encoding dictionary from the ph_token_to_int.json file\n",
    "with open('ph_token_to_int.json', 'r') as file:\n",
    "    ph_token_to_int = json.load(file)\n",
    "\n",
    "# Your input sequence\n",
    "input_sequence = \"AP n ei f a g e n a j i f u y u a p u AP n ei f a g e n a l e y u d ao en AP\"\n",
    "\n",
    "# Split the input sequence into individual phonemes\n",
    "input_phonemes = input_sequence.split()\n",
    "\n",
    "# Convert the phonemes to their corresponding integers using the encoding dictionary\n",
    "encoded_sequence = [ph_token_to_int[phoneme] for phoneme in input_phonemes]\n",
    "\n",
    "\n",
    "# Reshape the sequence to be 2D as expected by the encoder\n",
    "encoded_sequence_array = np.array(encoded_sequence).reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the sequence using the loaded encoder\n",
    "encoded_sequence_onehot = encoder.transform(encoded_sequence_array)\n",
    "\n",
    "padded_sequence = pad_sequences([encoded_sequence_onehot], maxlen=4449, padding='post', dtype='float')\n",
    "\n",
    "if padded_sequence.shape[2] != 123:\n",
    "    # Handle error: The one-hot encoded sequence does not match the number of features the model expects\n",
    "    # This may involve adding columns of zeros or otherwise adjusting the sequence to match the expected number of features\n",
    "    # For example:\n",
    "    new_shape = (padded_sequence.shape[0], 4449, 123)\n",
    "    new_padded_sequence = np.zeros(new_shape)\n",
    "    new_padded_sequence[:, :, :65] = padded_sequence  # Assuming the first 65 features match\n",
    "    padded_sequence = new_padded_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 226ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(padded_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Note Sequence: [[38 20 20 ... 20 20 20]]\n",
      "4449\n",
      "Inversed ph_dur: [0.06198255 0.06043197 0.05869877 ... 0.044998   0.044998   0.044998  ]\n",
      "4449\n",
      "Inversed f0_seq: [232.87163 232.66498 232.30423 ... 229.8     229.8     229.8    ]\n",
      "4449\n"
     ]
    }
   ],
   "source": [
    "# predicted_ph_dur, predicted_f0_seq, predicted_note_seq_encoded = predictions\n",
    "note_seq_pred = np.argmax(predictions[..., -num_note_categories:], axis=-1)  # Replace num_note_categories with the actual number\n",
    "\n",
    "ph_dur_pred_scaled = predictions[..., 1]  # Replace some_ph_dur_index with the actual index/indices\n",
    "note_seq_pred_scaled = predictions[..., 2]  # Replace some_ph_dur_index with the actual index/indices\n",
    "f0_seq_pred_scaled = predictions[..., 3]  # Replace some_f0_seq_index with the actual index/indices\n",
    "\n",
    "# Reshape the predictions to match the scaler's expected input\n",
    "ph_dur_pred_scaled = ph_dur_pred_scaled.reshape(-1, 1)\n",
    "note_seq_pred_scaled = note_seq_pred_scaled.reshape(-1, 1)\n",
    "f0_seq_pred_scaled = f0_seq_pred_scaled.reshape(-1, 1)\n",
    "\n",
    "# Use the scaler's inverse_transform method\n",
    "ph_dur_pred = scaler_ph_dur.inverse_transform(ph_dur_pred_scaled).flatten()\n",
    "f0_seq_pred = scaler_f0_seq.inverse_transform(f0_seq_pred_scaled).flatten()\n",
    "\n",
    "note_seq_pred = np.argmax(predictions[..., -num_note_categories:], axis=-1)  # Use the actual number of note categories\n",
    "\n",
    "# # Now you can print or return the decoded predictions\n",
    "# print(\"Decoded ph_dur:\", decoded_ph_dur_trimmed)\n",
    "# print(len(decoded_ph_dur_trimmed[0]))\n",
    "# if decoded_f0_seq is not None:\n",
    "#     print(\"Decoded f0_seq:\", decoded_f0_seq_trimmed)\n",
    "#     print(len(decoded_f0_seq_trimmed[0]))\n",
    "# print(\"Decoded note_seq_encoded:\", decoded_note_seq_encoded_trimmed)\n",
    "# print(len(decoded_note_seq_encoded_trimmed[0]))\n",
    "# Print the decoded sequences\n",
    "print('Decoded Note Sequence:', note_seq_pred)\n",
    "print(len(note_seq_pred[0]))\n",
    "print('Inversed ph_dur:', ph_dur_pred)\n",
    "print(len(ph_dur_pred))\n",
    "print('Inversed f0_seq:', f0_seq_pred)\n",
    "print(len(f0_seq_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Removing zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6352285332977772\n",
      "f0 size 327\n",
      "Decoded ph_dur without trailing zeros: [0.06198255 0.06043197 0.05869877 0.05523891 0.04858048 0.04534776\n",
      " 0.04500405 0.04499801 0.044998   0.044998   0.044998   0.044998\n",
      " 0.044998   0.044998   0.044998   0.044998   0.044998   0.044998\n",
      " 0.044998   0.044998   0.044998   0.044998   0.044998   0.044998\n",
      " 0.044998   0.044998   0.044998   0.044998   0.044998   0.044998\n",
      " 0.044998   0.044998   0.044998   0.044998   0.044998  ]\n",
      "35\n",
      "Decoded f0_seq without trailing zeros: [232.87163, 232.66498, 232.30423, 231.74814, 230.56653, 229.8877, 229.80173, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8, 229.8]\n",
      "327\n",
      "Decoded note_seq_encoded without trailing zeros: [38 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n",
      " 20 20 20 20 20 20 20 20 20 20 20]\n",
      "35\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "f0_timestep = 0.005\n",
    "f0_constant = 205.1\n",
    "\n",
    "note_seq_pred = note_seq_pred[0]\n",
    "\n",
    "def remove_trailing_zeros(sequence):\n",
    "    # Find the last non-zero element in the sequence\n",
    "    last_non_zero = -1\n",
    "    for i in range(len(sequence) - 1, -1, -1):\n",
    "        if sequence[i] != 0:\n",
    "            last_non_zero = i\n",
    "            break\n",
    "    # Slice the sequence to remove trailing zeros\n",
    "    return sequence[:last_non_zero + 1] if last_non_zero != -1 else sequence\n",
    "\n",
    "# Apply this function to each of your predicted sequences\n",
    "# decoded_ph_dur_no_zeros = remove_trailing_zeros(ph_dur_pred)\n",
    "# decoded_f0_seq_no_zeros = remove_trailing_zeros(f0_seq_pred)\n",
    "# decoded_note_seq_encoded_no_zeros = remove_trailing_zeros(note_seq_pred)\n",
    "\n",
    "if len(ph_dur_pred) > len(input_phonemes):\n",
    "        # Truncate the sequence if it's longer\n",
    "    ph_dur_pred = ph_dur_pred[:len(input_phonemes)]\n",
    "elif len(ph_dur_pred) < len(input_phonemes):\n",
    "    # Pad the sequence with zeros if it's shorter\n",
    "    ph_dur_pred = np.pad(ph_dur_pred, (0, len(input_phonemes) - len(ph_dur_pred)), 'constant')\n",
    "    \n",
    "if len(note_seq_pred) > len(input_phonemes):\n",
    "    # Truncate the sequence if it's longer\n",
    "    note_seq_pred = note_seq_pred[:len(input_phonemes)]\n",
    "elif len(note_seq_pred) < len(input_phonemes):\n",
    "    # Pad the sequence with zeros if it's shorter\n",
    "    note_seq_pred = np.pad(note_seq_pred, (0, len(input_phonemes) - len(ph_dur_pred)), 'constant')\n",
    "\n",
    "\n",
    "total_time = sum(ph_dur_pred)\n",
    "\n",
    "print(total_time)\n",
    "f0_size = int(total_time / f0_timestep)\n",
    "print('f0 size', f0_size)\n",
    "# Replace non-positive values with the specified constant\n",
    "decoded_f0_seq_no_zeros = [x if x > 0 else f0_constant for x in f0_seq_pred]\n",
    "\n",
    "# Pad sequence with the constant value if it's shorter than the target length\n",
    "if len(decoded_f0_seq_no_zeros) < f0_size:\n",
    "    decoded_f0_seq_no_zeros = np.pad(decoded_f0_seq_no_zeros, (0, f0_size - len(decoded_f0_seq_no_zeros)), 'constant', constant_values=(f0_constant,))\n",
    "\n",
    "# Or truncate if it's longer than the target length (just for safety)\n",
    "elif len(decoded_f0_seq_no_zeros) > f0_size:\n",
    "    decoded_f0_seq_no_zeros = decoded_f0_seq_no_zeros[:f0_size]\n",
    "\n",
    "# Now you have your sequences with trailing zeros removed\n",
    "print(\"Decoded ph_dur without trailing zeros:\", ph_dur_pred)\n",
    "print(len(ph_dur_pred))\n",
    "if decoded_f0_seq_no_zeros is not None:\n",
    "    print(\"Decoded f0_seq without trailing zeros:\", decoded_f0_seq_no_zeros)\n",
    "    print(len(decoded_f0_seq_no_zeros))\n",
    "print(\"Decoded note_seq_encoded without trailing zeros:\", note_seq_pred)\n",
    "print(len(note_seq_pred))\n",
    "print(len(input_phonemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Make it .ds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Phonetic Sequence: ['AP', 'n', 'ei', 'f', 'a', 'g', 'e', 'n', 'a', 'j', 'i', 'f', 'u', 'y', 'u', 'a', 'p', 'u', 'AP', 'n', 'ei', 'f', 'a', 'g', 'e', 'n', 'a', 'l', 'e', 'y', 'u', 'd', 'ao', 'en', 'AP']\n",
      "Decoded Note Sequence: ['G#3', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4', 'F#4']\n"
     ]
    }
   ],
   "source": [
    "# use input_phonemes\n",
    "\n",
    "# Load the token-to-int mappings from the JSON files\n",
    "def load_mapping(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        token_to_int = json.load(f)\n",
    "    # Invert the dictionary to create an int-to-token mapping\n",
    "    int_to_token = {v: k for k, v in token_to_int.items()}\n",
    "    return int_to_token\n",
    "\n",
    "# Decoding function using the mappings\n",
    "def decode_predictions(prediction_integers, mapping):\n",
    "    return [mapping.get(i, 'Unknown') for i in prediction_integers]\n",
    "\n",
    "note_int_to_token = load_mapping('note_token_to_int.json')\n",
    "\n",
    "predicted_note_seq_integers = note_seq_pred\n",
    "\n",
    "decoded_note_seq = decode_predictions(predicted_note_seq_integers, note_int_to_token)\n",
    "\n",
    "# Print or return the decoded sequences\n",
    "print(\"Decoded Phonetic Sequence:\", input_phonemes)\n",
    "print(\"Decoded Note Sequence:\", decoded_note_seq)\n",
    "\n",
    "ph_dur = ' '.join(map(str, ph_dur_pred))\n",
    "note_seq = ' '.join(map(str, decoded_note_seq))\n",
    "f0_seq = ' '.join(map(str, decoded_f0_seq_no_zeros))\n",
    "\n",
    "file = {\n",
    "    'ph_seq': input_sequence,\n",
    "    'ph_dur': ph_dur,\n",
    "    'note_seq': note_seq,\n",
    "    'f0_seq': f0_seq,\n",
    "    'f0_timestep': f0_timestep\n",
    "}\n",
    "\n",
    "with open('rnn_output.ds', 'w') as json_file:\n",
    "    json.dump(file, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectVivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
