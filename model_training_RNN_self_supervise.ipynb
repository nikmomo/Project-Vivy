{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Use: *RNN w/ LSTM*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import ast\n",
    "import gc\n",
    "\n",
    "file_path = 'normalized_output.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4022, 4450, 65)\n",
      "y_ph_dur shape: (4022, 4450, 1)\n",
      "y_note_seq_encoded shape: (4022, 4450, 56)\n",
      "y_f0_seq shape: (4022, 4450, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert string representations of lists to actual lists\n",
    "data['ph_seq_encoded'] = data['ph_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['ph_dur'] = data['ph_dur'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "data['f0_seq'] = data['f0_seq'].apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else x)  # Handle NaN for f0_seq\n",
    "data['note_seq_encoded'] = data['note_seq_encoded'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Normalize the float arrays\n",
    "scaler_ph_dur = MinMaxScaler(feature_range=(0, 1))\n",
    "# Normalize ph_dur assuming they are already in the correct format\n",
    "data['ph_dur'] = [scaler_ph_dur.fit_transform(np.array(seq).reshape(-1, 1)).flatten() for seq in data['ph_dur']]\n",
    "\n",
    "# Only normalize f0_seq if it's not NaN\n",
    "scaler_f0_seq = MinMaxScaler(feature_range=(0, 1))\n",
    "data['f0_seq'] = [scaler_f0_seq.fit_transform(np.array(seq).reshape(-1, 1)).flatten() if seq is not np.nan else np.nan for seq in data['f0_seq']]\n",
    "\n",
    "# Find the maximum sequence length across all sequence columns\n",
    "max_sequence_length = max(\n",
    "    max(data['ph_seq_encoded'].apply(len)),\n",
    "    max(data['ph_dur'].apply(len)),\n",
    "    max([len(seq) for seq in data['f0_seq'] if seq is not np.nan]),  # Only consider non-NaN sequences\n",
    "    max(data['note_seq_encoded'].apply(len))\n",
    ")\n",
    "\n",
    "# Pad the sequences\n",
    "data['ph_seq_encoded'] = pad_sequences(data['ph_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "data['ph_dur'] = pad_sequences(data['ph_dur'], maxlen=max_sequence_length, padding='post', dtype='float').tolist()\n",
    "data['f0_seq'] = [pad_sequences([seq], maxlen=max_sequence_length, padding='post', dtype='float').flatten() if seq is not np.nan else np.full(max_sequence_length, np.nan) for seq in data['f0_seq']]\n",
    "data['note_seq_encoded'] = pad_sequences(data['note_seq_encoded'], maxlen=max_sequence_length, padding='post').tolist()\n",
    "\n",
    "# Flatten all sequences into a single list\n",
    "all_ph_seq = [item for sublist in data['ph_seq_encoded'] for item in sublist]\n",
    "\n",
    "# Convert to numpy array and reshape to be 2D\n",
    "all_ph_seq_array = np.array(all_ph_seq).reshape(-1, 1)\n",
    "all_ph_seq.clear()\n",
    "\n",
    "# Initialize and fit the OneHotEncoder on all sequences at once\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(all_ph_seq_array)\n",
    "\n",
    "# Now transform each sequence individually and store the transformed arrays\n",
    "data['ph_seq_encoded'] = [encoder.transform(np.array(seq).reshape(-1, 1)) for seq in data['ph_seq_encoded']]\n",
    "\n",
    "# Determine the number of categories for notes\n",
    "num_note_categories = max(data['note_seq_encoded'].apply(max)) + 1  # Assuming the sequences are zero-indexed\n",
    "\n",
    "# One-hot encode the note_seq_encoded\n",
    "data['note_seq_encoded'] = [to_categorical(seq, num_classes=num_note_categories) for seq in data['note_seq_encoded']]\n",
    "\n",
    "padded_note_seqs = np.array([pad_sequences([seq], maxlen=max_sequence_length, padding='post', value=0)[0] for seq in data['note_seq_encoded']])\n",
    "data['note_seq_encoded'] = list(padded_note_seqs)\n",
    "\n",
    "# After padding and encoding, convert to a numpy array\n",
    "X = np.array(data['ph_seq_encoded'].tolist())\n",
    "y_ph_dur = np.array(data['ph_dur'].tolist())\n",
    "y_note_seq_encoded = np.array(data['note_seq_encoded'].tolist())\n",
    "\n",
    "# Ensure that all arrays have three dimensions\n",
    "y_ph_dur = y_ph_dur[..., np.newaxis]  # shape (num_sequences, sequence_length, 1)\n",
    "\n",
    "# Handle missing f0_seq data\n",
    "mask_value = -1  # Define a mask value that does not appear in the data\n",
    "y_f0_seq = np.array([np.full(max_sequence_length, mask_value) if np.isnan(seq).all() else seq for seq in data['f0_seq']])\n",
    "\n",
    "# If y_f0_seq is 2D, also convert it to 3D\n",
    "if y_f0_seq.ndim == 2:\n",
    "    y_f0_seq = y_f0_seq[..., np.newaxis]  # shape (num_sequences, sequence_length, 1)\n",
    "\n",
    "# Check the shapes of the arrays to make sure they are all 3D and can be concatenated\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y_ph_dur shape: {y_ph_dur.shape}\")\n",
    "print(f\"y_note_seq_encoded shape: {y_note_seq_encoded.shape}\")\n",
    "print(f\"y_f0_seq shape: {y_f0_seq.shape}\")\n",
    "\n",
    "# Concatenate your inputs along the last axis to create a single input array for each sequence\n",
    "X_combined = np.concatenate([X, y_ph_dur, y_note_seq_encoded, y_f0_seq], axis=-1)\n",
    "\n",
    "\n",
    "# Shift the combined input to create the input and target pairs\n",
    "X_input = X_combined[:, :-1, :]  # All but the last time step\n",
    "X_target = X_combined[:, 1:, :]  # All but the first time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07759133\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.37810651]\n"
     ]
    }
   ],
   "source": [
    "# Free some memory here\n",
    "del all_ph_seq_array\n",
    "del padded_note_seqs\n",
    "del all_ph_seq\n",
    "del data\n",
    "del X\n",
    "del y_f0_seq\n",
    "del y_ph_dur\n",
    "del y_note_seq_encoded\n",
    "del X_combined\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Niko\\anaconda3\\envs\\ProjectVivy\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 1/63 [..............................] - ETA: 35:50 - loss: 5.3748 - accuracy: 6.6728e-05"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "input_layer = Input(shape=(X_input.shape[1], X_input.shape[2]))\n",
    "lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "output_layer = Dense(X_input.shape[2], activation='softmax')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the input and target sequences\n",
    "model.fit(X_input, X_target, epochs=10, batch_size=64)\n",
    "\n",
    "# Save the model\n",
    "model.save('model_self_supervised.h5')\n",
    "\n",
    "import joblib\n",
    "\n",
    "# # Save the scalers and encoders\n",
    "# joblib.dump(scaler_ph_dur, 'scaler_ph_dur.pkl')\n",
    "# joblib.dump(scaler_f0_seq, 'scaler_f0_seq.pkl')\n",
    "# joblib.dump(encoder, 'ph_seq_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validation\n",
    "Put user input and get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the encoding dictionary from the ph_token_to_int.json file\n",
    "with open('ph_token_to_int.json', 'r') as file:\n",
    "    ph_token_to_int = json.load(file)\n",
    "\n",
    "# Your input sequence\n",
    "input_sequence = \"AP n ei f a g e n a j i f u y u a p u AP n ei f a g e n a l e y u d ao en AP\"\n",
    "\n",
    "# Split the input sequence into individual phonemes\n",
    "input_phonemes = input_sequence.split()\n",
    "\n",
    "# Convert the phonemes to their corresponding integers using the encoding dictionary\n",
    "encoded_sequence = [ph_token_to_int[phoneme] for phoneme in input_phonemes]\n",
    "\n",
    "# Convert the sequence to a numpy array and pad it to the right length\n",
    "new_ph_seq_encoded = np.array([encoded_sequence])  # wrapping in a list to create a batch dimension\n",
    "new_ph_seq_encoded_padded = pad_sequences(new_ph_seq_encoded, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "print(encoded_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(user_input_padded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ph_dur, predicted_f0_seq, predicted_note_seq_encoded = predictions\n",
    "\n",
    "predicted_ph_dur_2d = predicted_ph_dur.reshape(-1, 1)\n",
    "\n",
    "# Apply inverse transformation\n",
    "decoded_ph_dur_2d = scaler_ph_dur.inverse_transform(predicted_ph_dur_2d)\n",
    "# print(predicted_ph_dur_2d)\n",
    "\n",
    "# Reshape it back to the original shape if needed\n",
    "decoded_ph_dur = decoded_ph_dur_2d.reshape(-1, max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "predicted_f0_seq_2d = predicted_f0_seq.reshape(-1, 1)\n",
    "\n",
    "# Reverse normalization for 'f0_seq', if it's not all NaNs\n",
    "if not np.isnan(y_f0_seq_train).all():\n",
    "    decoded_f0_seq_2d = scaler_f0_seq.inverse_transform(predicted_f0_seq_2d)\n",
    "else:\n",
    "    decoded_f0_seq_2d = None  # or a placeholder value if f0_seq was not predicted\n",
    "    \n",
    "decoded_f0_seq = decoded_f0_seq_2d.reshape(-1, max_sequence_length)\n",
    "\n",
    "# Convert predicted probabilities for 'note_seq_encoded' back to category indices\n",
    "decoded_note_seq_encoded = np.argmax(predicted_note_seq_encoded, axis=-1)  # If the last dimension contains the category probabilities\n",
    "\n",
    "# If the decoded sequences are padded, you may want to trim the padding off. For example:\n",
    "trim_padding = lambda seq, mask: seq[:np.where(seq == mask)[0][0] if np.where(seq == mask)[0].size > 0 else None]\n",
    "# Now you can trim the padding if your original sequences were padded\n",
    "decoded_ph_dur_trimmed = [seq[seq != mask_value] for seq in decoded_ph_dur]\n",
    "if decoded_f0_seq is not None:\n",
    "    decoded_f0_seq_trimmed = [trim_padding(seq, mask_value) for seq in decoded_f0_seq]\n",
    "decoded_note_seq_encoded_trimmed = [trim_padding(seq, mask_value) for seq in decoded_note_seq_encoded]\n",
    "\n",
    "# Now you can print or return the decoded predictions\n",
    "print(\"Decoded ph_dur:\", decoded_ph_dur_trimmed)\n",
    "print(len(decoded_ph_dur_trimmed[0]))\n",
    "if decoded_f0_seq is not None:\n",
    "    print(\"Decoded f0_seq:\", decoded_f0_seq_trimmed)\n",
    "    print(len(decoded_f0_seq_trimmed[0]))\n",
    "print(\"Decoded note_seq_encoded:\", decoded_note_seq_encoded_trimmed)\n",
    "print(len(decoded_note_seq_encoded_trimmed[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Removing zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_timestep = 0.005\n",
    "f0_constant = 205.1\n",
    "def remove_trailing_zeros(sequence):\n",
    "    # Find the last non-zero element in the sequence\n",
    "    last_non_zero = -1\n",
    "    for i in range(len(sequence) - 1, -1, -1):\n",
    "        if sequence[i] != 0:\n",
    "            last_non_zero = i\n",
    "            break\n",
    "    # Slice the sequence to remove trailing zeros\n",
    "    return sequence[:last_non_zero + 1] if last_non_zero != -1 else sequence\n",
    "\n",
    "# Apply this function to each of your predicted sequences\n",
    "decoded_ph_dur_no_zeros = remove_trailing_zeros(decoded_ph_dur_trimmed[0])\n",
    "decoded_f0_seq_no_zeros = remove_trailing_zeros(decoded_f0_seq_trimmed[0])\n",
    "decoded_note_seq_encoded_no_zeros = remove_trailing_zeros(decoded_note_seq_encoded_trimmed[0])\n",
    "\n",
    "if len(decoded_ph_dur_no_zeros) > len(input_phonemes):\n",
    "        # Truncate the sequence if it's longer\n",
    "    decoded_ph_dur_no_zeros = decoded_ph_dur_no_zeros[:len(input_phonemes)]\n",
    "elif len(decoded_ph_dur_no_zeros) < len(input_phonemes):\n",
    "    # Pad the sequence with zeros if it's shorter\n",
    "    decoded_ph_dur_no_zeros = np.pad(decoded_ph_dur_no_zeros, (0, len(input_phonemes) - len(decoded_ph_dur_no_zeros)), 'constant')\n",
    "\n",
    "total_time = sum(decoded_ph_dur_no_zeros)\n",
    "\n",
    "print(total_time)\n",
    "f0_size = int(total_time / f0_timestep)\n",
    "print(f0_size)\n",
    "# Replace non-positive values with the specified constant\n",
    "decoded_f0_seq_no_zeros = [x if x > 0 else f0_constant for x in decoded_f0_seq_no_zeros]\n",
    "\n",
    "# Pad sequence with the constant value if it's shorter than the target length\n",
    "if len(decoded_f0_seq_no_zeros) < f0_size:\n",
    "    decoded_f0_seq_no_zeros = np.pad(decoded_f0_seq_no_zeros, (0, f0_size - len(decoded_f0_seq_no_zeros)), 'constant', constant_values=(f0_constant,))\n",
    "\n",
    "# Or truncate if it's longer than the target length (just for safety)\n",
    "elif len(decoded_f0_seq_no_zeros) > f0_size:\n",
    "    decoded_f0_seq_no_zeros = decoded_f0_seq_no_zeros[:f0_size]\n",
    "\n",
    "# Now you have your sequences with trailing zeros removed\n",
    "print(\"Decoded ph_dur without trailing zeros:\", decoded_ph_dur_no_zeros)\n",
    "print(len(decoded_ph_dur_no_zeros))\n",
    "if decoded_f0_seq_no_zeros is not None:\n",
    "    print(\"Decoded f0_seq without trailing zeros:\", decoded_f0_seq_no_zeros)\n",
    "    print(len(decoded_f0_seq_no_zeros))\n",
    "print(\"Decoded note_seq_encoded without trailing zeros:\", decoded_note_seq_encoded_no_zeros)\n",
    "print(len(decoded_note_seq_encoded_no_zeros))\n",
    "print(len(input_phonemes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Make it .ds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use input_phonemes\n",
    "\n",
    "# Load the token-to-int mappings from the JSON files\n",
    "def load_mapping(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        token_to_int = json.load(f)\n",
    "    # Invert the dictionary to create an int-to-token mapping\n",
    "    int_to_token = {v: k for k, v in token_to_int.items()}\n",
    "    return int_to_token\n",
    "\n",
    "# Decoding function using the mappings\n",
    "def decode_predictions(prediction_integers, mapping):\n",
    "    return [mapping.get(i, 'Unknown') for i in prediction_integers]\n",
    "\n",
    "note_int_to_token = load_mapping('note_token_to_int.json')\n",
    "\n",
    "predicted_note_seq_integers = decoded_note_seq_encoded_no_zeros\n",
    "\n",
    "decoded_note_seq = decode_predictions(predicted_note_seq_integers, note_int_to_token)\n",
    "\n",
    "# Print or return the decoded sequences\n",
    "print(\"Decoded Phonetic Sequence:\", input_phonemes)\n",
    "print(\"Decoded Note Sequence:\", decoded_note_seq)\n",
    "\n",
    "file = {\n",
    "    'ph_seq': input_phonemes,\n",
    "    'ph_dur': decoded_ph_dur_no_zeros,\n",
    "    'note_seq': decoded_note_seq,\n",
    "    'f0_seq': decoded_f0_seq_no_zeros\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectVivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
