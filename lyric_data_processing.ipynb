{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Enviornment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "folder_path = './data'\n",
    "output_path = 'output.csv' # output of non-normalized file\n",
    "norm_output_path = 'normalized_output.csv' # output of normalized file\n",
    "\n",
    "\n",
    "# this is use for convert opencpop txt to .ds for training\n",
    "txt_file_path = './data/transcriptions.txt' #opencpop training file\n",
    "ds_file_path = './data/opencpop.ds' # opencpop output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "Process the file and make them to a csv file for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Turn opencpop into .ds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been converted and written to ./data/opencpop.ds\n"
     ]
    }
   ],
   "source": [
    "# Function to read the .txt file and process each segment\n",
    "def process_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        segments = file.read().split('\\n')\n",
    "\n",
    "    ds_data = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            parts = segment.split('|')\n",
    "            if len(parts) >= 5:\n",
    "                ds_entry = {\n",
    "                    'ph_seq': parts[2].strip(),\n",
    "                    'note_seq': parts[3].strip(),\n",
    "                    'ph_dur': parts[4].strip()\n",
    "                }\n",
    "                ds_data.append(ds_entry)\n",
    "\n",
    "    return ds_data\n",
    "\n",
    "# Process the file\n",
    "ds_data = process_txt_file(txt_file_path)\n",
    "\n",
    "# Convert to .ds format (JSON) and write to a file\n",
    "with open(ds_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(ds_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data has been converted and written to {ds_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Process all .ds file to dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_sampleshort_origin.ds processed\n",
      "00_我多想说再见啊.ds processed\n",
      "01_逍遥仙.ds processed\n",
      "02_一半一半.ds processed\n",
      "04_仙瑶.ds processed\n",
      "06_不谓侠.ds processed\n",
      "opencpop.ds processed\n",
      "samples_左手指月改.ds processed\n",
      "samples_能解答一切的答案.ds processed\n",
      "samples_这么可爱真是抱歉.ds processed\n",
      "仙瑶.ds processed\n",
      "Data has been written to output.csv\n"
     ]
    }
   ],
   "source": [
    "def read_ds_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Process the .ds file and flatten the data for CSV, including additional calculations\n",
    "def process_data(data):\n",
    "    processed_data = []\n",
    "    for segment in data:\n",
    "        # Calculate the sum of ph_dur\n",
    "        ph_dur_list = [float(dur) for dur in segment.get('ph_dur', '').split()]\n",
    "        sum_ph_dur = sum(ph_dur_list)\n",
    "\n",
    "        # Count the elements in sequences\n",
    "        count_ph_dur = len(segment.get('ph_dur', '').split())\n",
    "        count_ph_seq = len(segment.get('ph_seq', '').split())\n",
    "        count_note_seq = len(segment.get('note_seq', '').split())\n",
    "        count_f0_seq = len(segment.get('f0_seq', '').split())\n",
    "\n",
    "        # Calculate total f0_time\n",
    "        f0_timestep = float(segment.get('f0_timestep', 0))\n",
    "        total_f0_time = f0_timestep * count_f0_seq\n",
    "\n",
    "        flattened_segment = {\n",
    "            'offset': segment.get('offset', ''),\n",
    "            'ph_seq': segment.get('ph_seq', ''),\n",
    "            'ph_dur': segment.get('ph_dur', ''),\n",
    "            'note_seq': segment.get('note_seq', ''),\n",
    "            'f0_seq': segment.get('f0_seq', ''),\n",
    "            'f0_timestep': segment.get('f0_timestep', ''),\n",
    "            'sum_ph_dur': sum_ph_dur,\n",
    "            'count_ph_dur': count_ph_dur,\n",
    "            'count_ph_seq': count_ph_seq,\n",
    "            'count_note_seq': count_note_seq,\n",
    "            'count_f0_seq': count_f0_seq,\n",
    "            'total_f0_time': total_f0_time\n",
    "        }\n",
    "        processed_data.append(flattened_segment)\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# Store all processed data from each file\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each .ds file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.ds'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data = read_ds_file(file_path)\n",
    "        all_data.extend(process_data(data))\n",
    "        print(f'{filename} processed')\n",
    "\n",
    "# Write the aggregated data to a CSV file\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=all_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    for data in all_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"Data has been written to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(output_path)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Tokenize the sequences\n",
    "df['ph_seq'] = df['ph_seq'].apply(lambda x: x.split())\n",
    "df['note_seq'] = df['note_seq'].apply(lambda x: x.split())\n",
    "df['ph_dur'] = df['ph_dur'].apply(lambda x: [float(i) for i in x.split()])\n",
    "# df['f0_seq'] = df['f0_seq'].apply(lambda x: [float(i) for i in x.split()] if isinstance(x, str) else x)\n",
    "\n",
    "# Function to scale an individual list\n",
    "def scale_list(lst):\n",
    "    # Reshape the list to fit the scaler's expected input shape\n",
    "    reshaped = np.array(lst).reshape(-1, 1)\n",
    "    # Scale the list\n",
    "    scaled = scaler.fit_transform(reshaped).flatten()\n",
    "    return scaled.tolist()\n",
    "\n",
    "# # Fit the scaler to the data and transform it\n",
    "# df['ph_dur'] = [scale_list(x) for x in df['ph_dur']]\n",
    "\n",
    "# Get unique tokens and create a mapping to integers for ph_seq\n",
    "unique_ph_tokens = set(token for seq in df['ph_seq'] for token in seq)\n",
    "ph_token_to_int = {token: i for i, token in enumerate(unique_ph_tokens, start=1)}\n",
    "\n",
    "with open('note_token_to_int.json', 'r') as file:\n",
    "    note_token_to_int = json.load(file)\n",
    "\n",
    "# Save the ph_seq token-to-int mapping to a JSON file\n",
    "with open('ph_token_to_int.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(ph_token_to_int, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Integer encode the sequences\n",
    "df['ph_seq_encoded'] = df['ph_seq'].apply(lambda seq: [ph_token_to_int[token] for token in seq])\n",
    "df['note_seq_encoded'] = df['note_seq'].apply(lambda seq: [note_token_to_int[token] for token in seq])\n",
    "\n",
    "df['note_seq_encoded'] = [scale_list(x) for x in df['note_seq_encoded']]\n",
    "\n",
    "# get the max size\n",
    "max_length = {df['ph_seq_encoded'].apply(len).max(), \n",
    "              df['note_seq_encoded'].apply(len).max(),\n",
    "              df['ph_dur'].apply(len).max()}\n",
    "max_length = max(max_length)\n",
    "\n",
    "# Function to pad sequences with the average value\n",
    "def pad_sequence(seq):\n",
    "    avg_value = np.mean(seq)\n",
    "    return list(seq) + [avg_value] * (max_length - len(seq))\n",
    "\n",
    "# Apply the padding function to each sequence\n",
    "df['ph_seq_encoded'] = df['ph_seq_encoded'].apply(pad_sequence)\n",
    "df['note_seq_encoded'] = df['note_seq_encoded'].apply(pad_sequence)\n",
    "df['ph_dur'] = df['ph_dur'].apply(pad_sequence)\n",
    "df['ph_dur'] = pd.array(df['ph_dur'])\n",
    "\n",
    "df.drop(['ph_seq', 'note_seq', 'offset', 'f0_timestep', 'f0_seq', 'count_ph_dur','count_ph_seq', 'count_note_seq','count_f0_seq','total_f0_time','sum_ph_dur'], axis=1, inplace=True)\n",
    "\n",
    "df.to_csv(norm_output_path, index=False)\n",
    "\n",
    "# Save the scaler to a file\n",
    "with open('note_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decoding\n",
    "decode the result after generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ph_seq token-to-int mapping from a JSON file\n",
    "with open('ph_token_to_int.json', 'r', encoding='utf-8') as f:\n",
    "    ph_token_to_int = json.load(f)\n",
    "\n",
    "# Load the note_seq token-to-int mapping from a JSON file\n",
    "with open('note_token_to_int.json', 'r', encoding='utf-8') as f:\n",
    "    note_token_to_int = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectVivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
